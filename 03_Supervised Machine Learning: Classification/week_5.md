# WEEK 5 QUIZ

## Q1. The term Bagging stands for bootstrap aggregating.
`True`

## Q2. This is the best way to choose the number of trees to build on a Bagging ensemble.

`Tune number of trees as a hyperparameter that needs to be optimized`

## Q3. Which type of Ensemble modeling approach is NOT a special case of model averaging?
`Boosting methods`
The Pasting method of Bootstrap aggregation


## Q4. What is an ensemble model that needs you to look at out of bag error?
`Random Forest`

## Q5. What is the main condition to use stacking as ensemble method?

`Models need to output predicted probabilities`

## Q6. This tree ensemble method only uses a subset of the features for each tree:

`Random Forest`

## Q7. Order these tree ensembles in order of most randomness to least randomness:
`Random Trees, Random Forest, Bagging`

## Q8. This is an ensemble model that does not use bootstrapped samples to fit the base trees, takes residuals into account, and fits the base trees iteratively:
`Boosting`

## Q9. When comparing the two ensemble methods Bagging and Boosting, what is one characteristic of Boosting?
`Fits entire data set`

## Q10. What is the most frequently discussed loss function in boosting algorithms?

`0-1 Loss Function`


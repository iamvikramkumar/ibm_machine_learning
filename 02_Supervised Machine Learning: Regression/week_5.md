# WEEK 5 QUIZ

## Q1. When working with regularization, what is the view that illuminates the actual optimization problem and shows why LASSO generally zeros out coefficients?
   - Answer: Geometric view

## Q2. When working with regularization, what is the view that recalibrates our understanding of LASSO and Ridge, as a base problem, where coefficients have particular prior distributions?
   - Answer: Probabilistic view

## Q3. When working with regularization, what is the logical view of how to achieve the goal of reducing complexity?
   - Answer: Analytical view

## Q4. All of the following statements about Regularization are TRUE except:
   - Answer: Features should rarely or never be scaled prior to implementing regularization.

## Q5. When working with regularization and using the geometric formulation, what is found at the intersection of the penalty boundary and a contour of the traditional OLS cost function surface?
   - Answer: The cost function minimum

## Q6. Which statement under the Probabilistic View is correct?
   - Answer: Regularization imposes certain priors on the regression coefficients.

## Q7. Increasing L2/L1 penalties force coefficients to be smaller, restricting their plausible range. This statement is part of what View?
   - Answer: Analytic View 

## Q8. What does a higher lambda term mean in Regularization technique?
   - Answer: Higher lambda decreases variance, means smaller coefficients.     


## Q9. What concept/s under Probabilistic View is/are True?
   - Answer: All of the above (We can derive the posterior probability by knowing the probability of the target and the prior distribution, The prior distribution is derived from independent draws of a prior coefficient density function that we choose when regularizing, L2 (ridge) regularization imposes a Gaussian prior on the coefficients, while L1 (lasso) regularization imposes a Laplacian prior.)

## Q10. What statement is True?
    - Answer: We reduce the complexity of the model by minimizing the error on our training set. 
